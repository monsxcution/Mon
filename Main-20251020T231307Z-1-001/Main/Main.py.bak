import os
import re
import json
import uuid
import time
import shutil
import asyncio
import sqlite3
import webbrowser
import threading
import sys
import winshell
from threading import Thread
from flask import (
    Flask,
    render_template,
    request,
    jsonify,
    Blueprint,
    flash,
    redirect,
    url_for,
    send_from_directory,
)
from werkzeug.utils import secure_filename
from telethon.sync import TelegramClient
from telethon.tl.types import PeerChannel
from telethon.errors import (
    SessionPasswordNeededError,
    FloodWaitError,
    UserBannedInChannelError,
    UsernameInvalidError,
    UsernameOccupiedError,
)
from telethon.tl.functions.channels import JoinChannelRequest
from telethon.tl.functions.messages import SendMessageRequest, SendReactionRequest
from telethon.tl.functions.account import UpdateProfileRequest, UpdateUsernameRequest
from PIL import Image, ImageDraw
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup, NavigableString
import pystray
from itertools import cycle
import random


# --- CẤU HÌNH ỨNG DỤNG FLASK ---
APP_ROOT = os.path.dirname(os.path.abspath(__file__))
app = Flask(__name__, template_folder=".")
app.secret_key = "your_super_secret_key_for_flask_app"

# Cấu hình để tự động reload templates và static files
app.config['TEMPLATES_AUTO_RELOAD'] = True
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0

# --- CẤU HÌNH FILE DỮ LIỆU ---
# TODO: These JSON file constants can be removed after the one-time migration is confirmed successful.
DATA_DIR = os.path.join(APP_ROOT, "data")
ACCOUNTS_DATA_FILE = os.path.join(DATA_DIR, "accounts.json")
TYPES_DATA_FILE = os.path.join(DATA_DIR, "password_types.json")
NOTES_DATA_FILE = os.path.join(DATA_DIR, "reminders.json")
FACEBOOK_ACCOUNTS_FILE = os.path.join(DATA_DIR, "facebook_accounts.json")
DATABASE_PATH = os.path.join(DATA_DIR, "Data.db")
PROXIES_FILE = os.path.join(DATA_DIR, "proxies.json")
UPLOAD_FOLDER = os.path.join(APP_ROOT, "data", "uploaded_sessions")
IMAGE_EDIT_FOLDER = os.path.join(DATA_DIR, "image_editor_files")
SOUNDS_FOLDER = os.path.join(DATA_DIR, "sounds")
DASHBOARD_SETTINGS_FILE = os.path.join(DATA_DIR, "dashboard_settings.json")
app.config["UPLOAD_FOLDER"] = UPLOAD_FOLDER
app.config["IMAGE_EDIT_FOLDER"] = IMAGE_EDIT_FOLDER
app.config["SOUNDS_FOLDER"] = SOUNDS_FOLDER

# --- BIẾN TOÀN CỤC CHO TELEGRAM ---
TASKS = {}
API_ID, API_HASH = 28610130, "eda4079a5b9d4f3f88b67dacd799f902"
ADMIN_SESSION_FOLDER = "Adminsession"


# --- KHỞI TẠO DATABASE & FILES ---
def init_database():
    # Telegram
    if not os.path.exists(UPLOAD_FOLDER):
        os.makedirs(UPLOAD_FOLDER)
    conn = get_db_connection()
    conn.execute(
        "CREATE TABLE IF NOT EXISTS session_groups (id INTEGER PRIMARY KEY, name TEXT NOT NULL UNIQUE, folder_path TEXT NOT NULL)"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS session_metadata (id INTEGER PRIMARY KEY, group_id INTEGER NOT NULL, filename TEXT NOT NULL, full_name TEXT, username TEXT, is_live BOOLEAN, status_text TEXT, last_checked TIMESTAMP, FOREIGN KEY (group_id) REFERENCES session_groups (id), UNIQUE (group_id, filename))"
    )
    conn.execute(
        "CREATE TABLE IF NOT EXISTS task_configs (task_name TEXT PRIMARY KEY, config_json TEXT NOT NULL)"
    )
    try:
        conn.execute("SELECT username FROM session_metadata LIMIT 1")
    except sqlite3.OperationalError:
        conn.execute("ALTER TABLE session_metadata ADD COLUMN username TEXT")
    
    # Passwords & Types
    conn.execute(
        """CREATE TABLE IF NOT EXISTS password_types (
            name TEXT PRIMARY KEY,
            color TEXT NOT NULL DEFAULT '#6c757d'
        )"""
    )
    conn.execute(
        """CREATE TABLE IF NOT EXISTS passwords (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            type TEXT,
            username TEXT,
            password TEXT,
            two_fa TEXT,
            notes TEXT,
            FOREIGN KEY (type) REFERENCES password_types (name)
        )"""
    )
    # Notes
    conn.execute(
        """CREATE TABLE IF NOT EXISTS notes (
            id TEXT PRIMARY KEY,
            title_html TEXT,
            content_html TEXT,
            due_time TEXT,
            status TEXT,
            modified_at TEXT
        )"""
    )
    
    try:
        conn.execute("SELECT is_marked FROM notes LIMIT 1")
    except sqlite3.OperationalError:
        print("Adding 'is_marked' column to notes table...")
        conn.execute("ALTER TABLE notes ADD COLUMN is_marked INTEGER DEFAULT 0")
    
    # Auto Seeding
    conn.execute(
        """CREATE TABLE IF NOT EXISTS auto_seeding_settings (
            id INTEGER PRIMARY KEY,
            is_enabled BOOLEAN NOT NULL DEFAULT 0,
            run_time TEXT,
            end_run_time TEXT,
            run_daily BOOLEAN NOT NULL DEFAULT 0,
            target_session_group_id INTEGER,
            last_run_timestamp TEXT,
            task_name TEXT NOT NULL DEFAULT 'seedingGroup'
        )"""
    )
    
    conn.commit()
    conn.close()

    # Password
    if not os.path.exists(ACCOUNTS_DATA_FILE):
        with open(ACCOUNTS_DATA_FILE, "w", encoding="utf-8") as f:
            json.dump([], f)
    if not os.path.exists(TYPES_DATA_FILE):
        default_types = ["Gmail", "Facebook", "Game", "GitHub", "Khác"]
        with open(TYPES_DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(default_types, f, indent=4)

    # Facebook
    if not os.path.exists(FACEBOOK_ACCOUNTS_FILE):
        with open(FACEBOOK_ACCOUNTS_FILE, "w", encoding="utf-8") as f:
            json.dump([], f)

    # Image Editor
    if not os.path.exists(IMAGE_EDIT_FOLDER):
        os.makedirs(IMAGE_EDIT_FOLDER)


def migrate_json_to_sqlite():
    migration_flag_file = os.path.join(DATA_DIR, 'migration_done.flag')
    if os.path.exists(migration_flag_file):
        return # Migration has already been done.

    print("Running one-time data migration from JSON to SQLite...")
    conn = get_db_connection()
    cursor = conn.cursor()

    # Migrate password_types.json
    try:
        if os.path.exists(TYPES_DATA_FILE):
            with open(TYPES_DATA_FILE, 'r', encoding='utf-8') as f:
                types_data = json.load(f)
                for type_obj in types_data:
                    if isinstance(type_obj, dict):
                        cursor.execute("INSERT OR IGNORE INTO password_types (name, color) VALUES (?, ?)", (type_obj['name'], type_obj['color']))
                    elif isinstance(type_obj, str): # Handle old format
                        cursor.execute("INSERT OR IGNORE INTO password_types (name) VALUES (?)", (type_obj,))
            print(f"Migrated {len(types_data)} password types.")
    except Exception as e:
        print(f"Error migrating password types: {e}")

    # Migrate accounts.json
    try:
        if os.path.exists(ACCOUNTS_DATA_FILE):
            with open(ACCOUNTS_DATA_FILE, 'r', encoding='utf-8') as f:
                accounts_data = json.load(f)
                for acc in accounts_data:
                    cursor.execute(
                        "INSERT INTO passwords (type, username, password, two_fa, notes) VALUES (?, ?, ?, ?, ?)",
                        (acc.get('type'), acc.get('username'), acc.get('password'), acc.get('2fa'), acc.get('notes'))
                    )
            print(f"Migrated {len(accounts_data)} passwords.")
    except Exception as e:
        print(f"Error migrating passwords: {e}")

    # Migrate reminders.json (notes)
    try:
        if os.path.exists(NOTES_DATA_FILE):
            with open(NOTES_DATA_FILE, 'r', encoding='utf-8') as f:
                notes_data = json.load(f)
                for note in notes_data:
                    cursor.execute(
                        "INSERT OR IGNORE INTO notes (id, title_html, content_html, due_time, status, modified_at) VALUES (?, ?, ?, ?, ?, ?)",
                        (note.get('id'), note.get('title_html'), note.get('content_html'), note.get('due_time'), note.get('status'), note.get('modified_at'))
                    )
            print(f"Migrated {len(notes_data)} notes.")
    except Exception as e:
        print(f"Error migrating notes: {e}")

    conn.commit()
    conn.close()
    
    # Create flag file to prevent re-running
    with open(migration_flag_file, 'w') as f:
        f.write(datetime.now().isoformat())
    print("Migration complete.")


# --- PASSWORD MANAGER: BLUEPRINT & LOGIC ---
password_bp = Blueprint("password", __name__, url_prefix="/password")

@password_bp.route("/add", methods=["POST"])
def add_account():
    conn = get_db_connection()
    conn.execute(
        "INSERT INTO passwords (type, username, password, two_fa, notes) VALUES (?, ?, ?, ?, ?)",
        (request.form["type"], request.form["username"], request.form["password"], request.form.get("2fa", ""), request.form["notes"]),
    )
    conn.commit()
    conn.close()
    flash("Đã thêm tài khoản thành công!", "password_success")
    return redirect(url_for("main_dashboard", tab="password"))

@password_bp.route("/update/<int:account_id>", methods=["POST"])
def update_account(account_id):
    conn = get_db_connection()
    conn.execute(
        "UPDATE passwords SET type = ?, username = ?, password = ?, two_fa = ?, notes = ? WHERE id = ?",
        (request.form["type"], request.form["username"], request.form["password"], request.form.get("2fa", ""), request.form["notes"], account_id),
    )
    conn.commit()
    conn.close()
    flash("Đã cập nhật tài khoản thành công!", "password_success")
    return redirect(url_for("main_dashboard", tab="password"))

@password_bp.route("/delete/<int:account_id>", methods=["POST"])
def delete_account(account_id):
    conn = get_db_connection()
    conn.execute("DELETE FROM passwords WHERE id = ?", (account_id,))
    conn.commit()
    conn.close()
    flash("Đã xóa tài khoản thành công!", "password_info")
    return redirect(url_for("main_dashboard", tab="password"))

@password_bp.route("/types/add", methods=["POST"])
def add_type():
    new_type_name = request.form.get("new_type", "").strip()
    if new_type_name:
        conn = get_db_connection()
        try:
            conn.execute("INSERT INTO password_types (name) VALUES (?)", (new_type_name,))
            conn.commit()
            flash(f'Đã thêm loại "{new_type_name}" thành công!', "password_success")
        except sqlite3.IntegrityError:
            flash(f'Loại "{new_type_name}" đã tồn tại!', "password_warning")
        finally:
            conn.close()
    return redirect(url_for("main_dashboard", tab="password"))

@password_bp.route("/types/delete", methods=["POST"])
def delete_type():
    type_to_delete = request.form.get("type_to_delete")
    if type_to_delete:
        conn = get_db_connection()
        conn.execute("DELETE FROM password_types WHERE name = ?", (type_to_delete,))
        conn.commit()
        conn.close()
        flash(f'Đã xóa loại "{type_to_delete}"!', "password_info")
    return redirect(url_for("main_dashboard", tab="password"))

@password_bp.route("/types/update_color", methods=["POST"])
def update_type_color():
    data = request.json
    type_name, new_color = data.get("name"), data.get("color")
    if not type_name or not new_color:
        return jsonify({"error": "Dữ liệu không hợp lệ"}), 400
    
    conn = get_db_connection()
    cursor = conn.execute("UPDATE password_types SET color = ? WHERE name = ?", (new_color, type_name))
    conn.commit()
    conn.close()
    
    if cursor.rowcount > 0:
        return jsonify({"success": True, "message": "Đã cập nhật màu."})
    else:
        return jsonify({"error": "Không tìm thấy loại"}), 404

# --- FACEBOOK MANAGER: BLUEPRINT & LOGIC ---
facebook_bp = Blueprint("facebook", __name__, url_prefix="/facebook")


def load_fb_accounts():
    if not os.path.exists(FACEBOOK_ACCOUNTS_FILE):
        return []
    with open(FACEBOOK_ACCOUNTS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_fb_accounts(accounts):
    with open(FACEBOOK_ACCOUNTS_FILE, "w", encoding="utf-8") as f:
        json.dump(accounts, f, indent=4, ensure_ascii=False)


def get_next_fb_id(accounts):
    return max([acc.get("id", 0) for acc in accounts] or [0]) + 1


@facebook_bp.route("/add", methods=["POST"])
def add_fb_account():
    accounts = load_fb_accounts()
    nickname = request.form.get("nickname", "").strip()
    url = request.form.get("url", "").strip()
    if not nickname or not url:
        flash("Nickname và URL Facebook là bắt buộc.", "facebook_danger")
        return redirect(url_for("main_dashboard", tab="facebook"))
    if not url.startswith("http://") and not url.startswith("https://"):
        url = "https://" + url
    if not re.match(r"https?://(www\.)?facebook\.com/.*", url):
        flash(
            "URL Facebook không hợp lệ. Vui lòng nhập link dạng https://facebook.com/...",
            "facebook_danger",
        )
        return redirect(url_for("main_dashboard", tab="facebook"))
    new_account = {
        "id": get_next_fb_id(accounts),
        "nickname": nickname,
        "url": url,
        "notes": request.form.get("notes", ""),
    }
    accounts.append(new_account)
    save_fb_accounts(accounts)
    flash("Đã thêm tài khoản Facebook thành công!", "facebook_success")
    return redirect(url_for("main_dashboard", tab="facebook"))


@facebook_bp.route("/update/<int:account_id>", methods=["POST"])
def update_fb_account(account_id):
    accounts = load_fb_accounts()
    account = next((acc for acc in accounts if acc.get("id") == account_id), None)
    if account:
        nickname = request.form.get("nickname", "").strip()
        url = request.form.get("url", "").strip()
        if not nickname or not url:
            flash("Nickname và URL Facebook là bắt buộc.", "facebook_danger")
            return redirect(url_for("main_dashboard", tab="facebook"))
        if not url.startswith("http://") and not url.startswith("https://"):
            url = "https://" + url
        if not re.match(r"https?://(www\.)?facebook\.com/.*", url):
            flash("URL Facebook không hợp lệ.", "facebook_danger")
            return redirect(url_for("main_dashboard", tab="facebook"))
        account.update(
            {
                "nickname": nickname,
                "url": url,
                "notes": request.form.get("notes", ""),
            }
        )
        save_fb_accounts(accounts)
        flash("Đã cập nhật tài khoản Facebook thành công!", "facebook_success")
    else:
        flash("Không tìm thấy tài khoản Facebook!", "facebook_danger")
    return redirect(url_for("main_dashboard", tab="facebook"))


@facebook_bp.route("/delete/<int:account_id>", methods=["POST"])
def delete_fb_account(account_id):
    accounts = load_fb_accounts()
    new_accounts = [acc for acc in accounts if acc.get("id") != account_id]
    if len(accounts) > len(new_accounts):
        save_fb_accounts(new_accounts)
        flash("Đã xóa tài khoản Facebook thành công!", "facebook_info")
    else:
        flash("Không tìm thấy tài khoản Facebook!", "facebook_danger")
    return redirect(url_for("main_dashboard", tab="facebook"))


# --- TELEGRAM MANAGER: BLUEPRINT & LOGIC ---
telegram_bp = Blueprint("telegram", __name__, url_prefix="/telegram")


def get_db_connection():
    conn = sqlite3.connect(DATABASE_PATH, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn


@telegram_bp.route("/api/groups", methods=["GET", "POST"])
def manage_groups():
    conn = get_db_connection()
    if request.method == "GET":
        groups = conn.execute("SELECT * FROM session_groups ORDER BY name").fetchall()
        conn.close()
        return jsonify([dict(row) for row in groups])
    if request.method == "POST":
        name = request.form.get("name")
        files = request.files.getlist("session_files")
        if not name or not files or files[0].filename == "":
            conn.close()
            return jsonify({"error": "Tên nhóm và file không được trống"}), 400
        group_folder_name = secure_filename(name)
        group_path = os.path.join(app.config["UPLOAD_FOLDER"], group_folder_name)
        if os.path.exists(group_path):
            conn.close()
            return jsonify({"error": f'Tên nhóm "{name}" đã tồn tại.'}), 409
        os.makedirs(group_path)
        for file in files:
            if file and file.filename.endswith(".session"):
                file.save(os.path.join(group_path, secure_filename(file.filename)))
        try:
            conn.execute(
                "INSERT INTO session_groups (name, folder_path) VALUES (?, ?)",
                (name, group_path),
            )
            conn.commit()
            return jsonify({"success": True, "message": "Tạo nhóm thành công"}), 201
        except sqlite3.IntegrityError:
            shutil.rmtree(group_path)
            return jsonify({"error": "Tên nhóm đã tồn tại trong DB."}), 409
        finally:
            conn.close()


@telegram_bp.route("/api/groups/<int:group_id>", methods=["DELETE"])
def delete_group(group_id):
    conn = get_db_connection()
    group = conn.execute(
        "SELECT folder_path FROM session_groups WHERE id = ?", (group_id,)
    ).fetchone()
    if group and os.path.exists(group["folder_path"]):
        shutil.rmtree(group["folder_path"])
    conn.execute("DELETE FROM session_metadata WHERE group_id = ?", (group_id,))
    conn.execute("DELETE FROM session_groups WHERE id = ?", (group_id,))
    conn.commit()
    conn.close()
    return jsonify({"success": True})


@telegram_bp.route("/api/groups/<int:group_id>/sessions", methods=["GET"])
def get_group_sessions(group_id):
    conn = get_db_connection()
    group = conn.execute(
        "SELECT folder_path FROM session_groups WHERE id = ?", (group_id,)
    ).fetchone()
    if not group:
        conn.close()
        return jsonify({"error": "Không tìm thấy nhóm"}), 404
    metadata_rows = conn.execute(
        "SELECT * FROM session_metadata WHERE group_id = ?", (group_id,)
    ).fetchall()
    conn.close()
    metadata_map = {row["filename"]: dict(row) for row in metadata_rows}
    sessions = []
    folder_path = group["folder_path"]
    if os.path.exists(folder_path):
        session_files = sorted(
            [f for f in os.listdir(folder_path) if f.endswith(".session")]
        )
        for i, filename in enumerate(session_files):
            meta = metadata_map.get(filename, {})
            phone_match = re.search(r"\+?\d{9,15}", filename.replace(".session", ""))
            phone = phone_match.group(0) if phone_match else filename
            sessions.append(
                {
                    "stt": i + 1,
                    "phone": phone,
                    "filename": filename,
                    "full_name": meta.get("full_name", "Chưa kiểm tra"),
                    "username": meta.get("username", ""),
                    "is_live": meta.get("is_live"),
                    "status_text": meta.get("status_text", "Sẵn sàng"),
                }
            )
    return jsonify(sessions)


@telegram_bp.route("/api/upload-admin-sessions", methods=["POST"])
def upload_admin_sessions():
    files = request.files.getlist("admin_session_files")
    if not files or not files[0].filename:
        return jsonify({"error": "Không có file nào được tải lên"}), 400
    admin_folder_path = os.path.join(app.config["UPLOAD_FOLDER"], ADMIN_SESSION_FOLDER)
    if not os.path.exists(admin_folder_path):
        os.makedirs(admin_folder_path)
    file_count = 0
    for file in files:
        if file and file.filename.endswith(".session"):
            file.save(os.path.join(admin_folder_path, secure_filename(file.filename)))
            file_count += 1
    if file_count == 0:
        return jsonify({"error": "Không có file .session hợp lệ"}), 400
    conn = get_db_connection()
    try:
        conn.execute(
            "INSERT INTO session_groups (name, folder_path) VALUES (?, ?) ON CONFLICT(name) DO UPDATE SET folder_path=excluded.folder_path",
            (ADMIN_SESSION_FOLDER, admin_folder_path),
        )
        conn.commit()
    finally:
        conn.close()
    return jsonify(
        {"success": True, "message": f"Đã tải lên {file_count} session admin."}
    )


@app.route('/telegram/api/groups/<int:group_id>/upload-sessions', methods=['POST'])
def upload_sessions_to_group(group_id):
    conn = get_db_connection()
    group = conn.execute("SELECT folder_path FROM session_groups WHERE id = ?", (group_id,)).fetchone()
    conn.close()
    if not group:
        return jsonify({"error": "Không tìm thấy nhóm"}), 404
    folder_path = group["folder_path"] if isinstance(group, dict) else group[0]
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    files = request.files.getlist('session_files')
    count = 0
    for file in files:
        if file and file.filename.endswith('.session'):
            file.save(os.path.join(folder_path, secure_filename(file.filename)))
            count += 1
    return jsonify({"success": True, "uploaded": count})


@telegram_bp.route("/api/config/<task_name>", methods=["GET", "POST"])
def manage_config(task_name):
    conn = get_db_connection()
    try:
        if request.method == "GET":
            row = conn.execute(
                "SELECT config_json FROM task_configs WHERE task_name = ?", (task_name,)
            ).fetchone()
            return jsonify(json.loads(row["config_json"]) if row else {})
        if request.method == "POST":
            config_data = request.get_json()
            conn.execute(
                "INSERT INTO task_configs (task_name, config_json) VALUES (?, ?) ON CONFLICT(task_name) DO UPDATE SET config_json=excluded.config_json",
                (task_name, json.dumps(config_data)),
            )
            conn.commit()
            return jsonify({"success": True})
    finally:
        conn.close()


# --- Helper cho proxy ---
def load_proxies():
    """
    Tải cấu hình proxy. Tự động nâng cấp từ định dạng cũ (list) sang định dạng mới (dict).
    """
    if not os.path.exists(PROXIES_FILE):
        return {"enabled": False, "proxies": []}
    try:
        with open(PROXIES_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, list):
                return {"enabled": True, "proxies": data}
            return {
                "enabled": data.get("enabled", False),
                "proxies": data.get("proxies", [])
            }
    except (json.JSONDecodeError, IOError):
        return {"enabled": False, "proxies": []}

def save_proxies(proxy_config):
    """
    Lưu cấu hình proxy (bao gồm trạng thái bật/tắt và danh sách proxy).
    """
    with open(PROXIES_FILE, "w", encoding="utf-8") as f:
        json.dump(proxy_config, f, indent=4, ensure_ascii=False)

def parse_proxy_string(proxy_str):
    if not proxy_str:
        return None
    try:
        parts = proxy_str.strip().split(':')
        if len(parts) == 4:
            return {
                'proxy_type': 'http',
                'addr': parts[0],
                'port': int(parts[1]),
                'username': parts[2],
                'password': parts[3]
            }
        elif len(parts) == 2:
            return {
                'proxy_type': 'http',
                'addr': parts[0],
                'port': int(parts[1])
            }
        return None
    except (ValueError, IndexError):
        return None

async def join_group_worker(session_path, links, *args, **kwargs):
    """Worker function for joining groups/channels."""
    proxy_info = kwargs.get("proxy_info")
    status = {"is_live": False, "full_name": "Lỗi", "username": "", "status_text": "Lỗi không xác định", "joined_count": 0}
    client = None
    if not links:
        status["status_text"] = "Không có link"
        return status
    try:
        proxy_dict = parse_proxy_string(proxy_info)
        client = TelegramClient(session_path, API_ID, API_HASH, proxy=proxy_dict)
        await client.connect()
        if not await client.is_user_authorized():
            status["status_text"] = "Session die"
            return status

        me = await client.get_me()
        status["is_live"] = True
        status["full_name"] = f"{me.first_name or ''} {me.last_name or ''}".strip()
        status["username"] = me.username or ""
        
        joined_count = 0
        for link in links:
            try:
                await client(JoinChannelRequest(link))
                joined_count += 1
                await asyncio.sleep(1)
            except (ValueError, TypeError):
                continue
            except (UserBannedInChannelError, FloodWaitError):
                continue
        status["status_text"] = f"Đã join {joined_count}/{len(links)} nhóm"
        status["joined_count"] = joined_count
    except Exception as e:
        status["status_text"] = str(e)[:50]
        # Preserve last known name if an error occurs mid-task
        if 'me' in locals() and me:
            status["full_name"] = f"{me.first_name or ''} {me.last_name or ''}".strip()
            status["username"] = me.username or ""
    finally:
        if client and client.is_connected():
            await client.disconnect()
    return status

async def seeding_group_worker(session_path, group_link, message, send_silent=False, *args, **kwargs):
    """Worker function for seeding groups with specific message and group."""
    proxy_info = kwargs.get("proxy_info")
    status = {"is_live": False, "full_name": "Lỗi", "username": "", "status_text": "Lỗi seeding"}
    client = None
    try:
        proxy_dict = parse_proxy_string(proxy_info)
        client = TelegramClient(session_path, API_ID, API_HASH, proxy=proxy_dict)
        await client.connect()
        if not await client.is_user_authorized():
            status["status_text"] = "Session die"
            return status

        me = await client.get_me()
        status["is_live"] = True
        status["full_name"] = f"{me.first_name or ''} {me.last_name or ''}".strip()
        status["username"] = me.username or ""
        
        try:
            await client(JoinChannelRequest(group_link))
        except Exception:
             pass # Continue even if join fails

        await client.send_message(group_link, message, silent=send_silent)
        status["status_text"] = "Đã gửi tin nhắn"
        
    except Exception as e:
        status["status_text"] = str(e)[:50]
        # Preserve last known name if an error occurs mid-task
        if 'me' in locals() and me:
            status["full_name"] = f"{me.first_name or ''} {me.last_name or ''}".strip()
            status["username"] = me.username or ""
    finally:
        if client and client.is_connected():
            await client.disconnect()
    return status

async def run_admin_task(admin_session_path, group_link, message):
    """A dedicated function to run the admin session task."""
    client = None
    try:
        # Admin does not use proxy in this logic
        client = TelegramClient(admin_session_path, API_ID, API_HASH)
        await client.connect()
        if not await client.is_user_authorized():
            print("Admin session is not authorized.")
            return

        # Ensure the admin is in the group before sending a message.
        try:
            await client(JoinChannelRequest(group_link))
        except Exception as join_error:
            # Log the error but continue, as the session might already be in the channel.
            print(f"Admin session join error (might be already in channel): {join_error}")

        await client.send_message(group_link, message)
        print(f"Admin sent message to {group_link}")
    except Exception as e:
        print(f"Admin task failed for group {group_link}: {e}")
    finally:
        if client and client.is_connected():
            await client.disconnect()

async def check_single_session_worker(session_path, *args, **kwargs):
    proxy_info = kwargs.get("proxy_info")
    status = {"is_live": False, "full_name": "Lỗi", "username": "", "status_text": "Error"}
    client = None
    try:
        proxy_dict = parse_proxy_string(proxy_info)
        client = TelegramClient(session_path, API_ID, API_HASH, proxy=proxy_dict)
        await client.connect()
        if await client.is_user_authorized():
            me = await client.get_me()
            status = {"is_live": True, "full_name": f"{me.first_name or ''} {me.last_name or ''}".strip(), "username": me.username or "", "status_text": "Live"}
        else:
            status["status_text"] = "Dead"
    except SessionPasswordNeededError:
        status["status_text"] = "2FA Enabled"
    except Exception as e:
        status["status_text"] = str(e)[:50]
    finally:
        if client and client.is_connected():
            await client.disconnect()
    return status

async def task_worker(task_id, group_id, session_path, filename, coro_func, *args, **kwargs):
    proxy_info = kwargs.get("proxy_info")
    status_result = await coro_func(session_path, *args, proxy_info=proxy_info)
    
    conn = get_db_connection()
    try:
        conn.execute(
            "INSERT INTO session_metadata (group_id, filename, full_name, username, is_live, status_text, last_checked) VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP) ON CONFLICT(group_id, filename) DO UPDATE SET full_name=excluded.full_name, username=excluded.username, is_live=excluded.is_live, status_text=excluded.status_text, last_checked=CURRENT_TIMESTAMP",
            (group_id, filename, status_result.get("full_name"), status_result.get("username"), status_result.get("is_live"), status_result.get("status_text"))
        )
        conn.commit()
    finally:
        conn.close()

    task = TASKS.get(task_id)
    if task:
        task["processed"] += 1
        if status_result.get("is_live"):
            task["success"] += 1
        else:
            task["failed"] += 1
        task["results"].append({"filename": filename, **status_result})

def run_task_in_thread(
    task_id, group_id, folder_path, filenames,
    core, delay_per_session, delay_between_batches,
    admin_enabled, admin_delay,
    worker_coro_func, *args, **kwargs
):
    async def main():
        task = TASKS.get(task_id)
        if not task or not folder_path:
            if task: task["status"] = "failed"
            return

        proxies = kwargs.get("proxies", [])
        config = args[0] if args else {}
        
        # Prepare list of tasks to run
        tasks_to_run = []
        for f in filenames:
            if not f:
                task["total"] = max(0, task["total"] - 1)
                continue
            session_file_path = os.path.join(folder_path, f)
            if os.path.exists(session_file_path):
                tasks_to_run.append((session_file_path, f))

        # Determine concurrency and batching logic based on task type
        is_seeding_task = task.get("task_name") == "seedingGroup"
        if is_seeding_task:
            group_links = config.get("group_links", [])
            if not group_links:
                task["status"] = "failed"; task["messages"].append("Lỗi: Seeding cần ít nhất 1 link nhóm.")
                return
            concurrency = len(group_links)
            group_cycler = cycle(group_links)
            scenario_cycler = cycle(config.get("messages", []))
        else:
            concurrency = core
        
        proxy_cycler = cycle(proxies) if proxies else cycle([None])
        admin_group_index = 0

        # Main execution loop, iterating in batches
        for i in range(0, len(tasks_to_run), concurrency):
            if task.get("status") == "stopped": break

            batch_files = tasks_to_run[i : i + concurrency]
            async_tasks = []

            # Staggered start loop for tasks within the batch
            for session_path, filename in batch_files:
                if task.get("status") == "stopped": break
                
                worker_args = []
                if is_seeding_task:
                    worker_args = [next(group_cycler), next(scenario_cycler), config.get('send_silent', False)]
                elif args: # For other tasks like joinGroup
                    worker_args = list(args)

                # Create the async task
                coro = task_worker(
                    task_id, group_id, session_path, filename,
                    worker_coro_func, *worker_args, proxy_info=next(proxy_cycler)
                )
                async_tasks.append(asyncio.create_task(coro))

                # Wait for the per-session delay before starting the next one
                if delay_per_session > 0:
                    await asyncio.sleep(delay_per_session)

            # Wait for all tasks in the current batch to complete
            await asyncio.gather(*async_tasks)

            # Admin Logic after each batch
            if is_seeding_task and admin_enabled and task.get("status") != "stopped":
                admin_session_file = config.get("admin_session_file")
                admin_messages = config.get("admin_messages", [])
                admin_folder = os.path.join(app.config["UPLOAD_FOLDER"], ADMIN_SESSION_FOLDER)
                admin_session_path = os.path.join(admin_folder, admin_session_file) if admin_session_file else None

                if admin_session_path and os.path.exists(admin_session_path) and admin_messages:
                    admin_target_group = group_links[admin_group_index]
                    admin_response = random.choice(admin_messages)
                    
                    if admin_delay > 0:
                        for j in range(admin_delay, 0, -1):
                            if task.get("status") == "stopped": break
                            task["messages"].append(f"Admin trả lời sau... {j}s")
                            await asyncio.sleep(1)

                    if task.get("status") != "stopped":
                        await run_admin_task(admin_session_path, admin_target_group, admin_response)
                        admin_group_index = (admin_group_index + 1) % len(group_links)

            # Delay between batches
            if i + concurrency < len(tasks_to_run) and task.get("status") != "stopped" and delay_between_batches > 0:
                for j in range(delay_between_batches, 0, -1):
                    if task.get("status") == "stopped": break
                    task["messages"].append(f"Đang chờ đợt tiếp... {j}s")
                    await asyncio.sleep(1)

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(main())
    finally:
        task = TASKS.get(task_id)
        if task and task.get("status") != "stopped":
            task["status"] = "completed"
        loop.close()


# --- API quản lý proxy ---
@telegram_bp.route("/api/proxies", methods=["GET"])
def get_proxies():
    return jsonify(load_proxies())

@telegram_bp.route("/api/proxies", methods=["POST"])
def update_proxies():
    data = request.json
    is_enabled = data.get("enabled", False)
    proxy_list_str = data.get("proxies", "")
    proxy_list = [line.strip() for line in proxy_list_str.splitlines() if line.strip()]
    
    proxy_config = {
        "enabled": is_enabled,
        "proxies": proxy_list
    }
    save_proxies(proxy_config)
    status_text = "Bật" if is_enabled else "Tắt"
    return jsonify({"success": True, "message": f"Đã lưu {len(proxy_list)} proxy. Trạng thái: {status_text}."})


# --- Sửa logic sử dụng proxy trong run_task ---
@telegram_bp.route("/api/run-task", methods=["POST"])
def run_task():
    data = request.get_json() or {}
    # Extract all new and existing parameters
    group_id = data.get("groupId")
    task_name = data.get("task")
    config = data.get("config", {})
    filenames = data.get("filenames", [])
    core = int(data.get("core", 5))
    delay_per_session = int(data.get("delay_per_session", 10))
    delay_between_batches = int(data.get("delay_between_batches", 600))
    admin_enabled = bool(data.get("admin_enabled", False))
    admin_delay = int(data.get("admin_delay", 10))

    if not all([group_id, task_name, filenames]):
        return jsonify({"error": "Dữ liệu không hợp lệ"}), 400

    conn = get_db_connection()
    group = conn.execute("SELECT folder_path FROM session_groups WHERE id = ?", (group_id,)).fetchone()
    conn.close()
    if not group or not group["folder_path"]:
        return jsonify({"error": "Không tìm thấy nhóm hoặc đường dẫn thư mục của nhóm không hợp lệ."}), 404
    
    proxy_config = load_proxies()
    proxies_to_use = proxy_config['proxies'] if proxy_config.get('enabled', False) else []

    task_id = str(uuid.uuid4())
    TASKS[task_id] = {
        "task_name": task_name, "group_id": group_id, "status": "running",
        "total": len(filenames), "processed": 0, "success": 0, "failed": 0,
        "results": [], "messages": []
    }
    
    worker_func, args = None, []
    if task_name == "check-live":
        worker_func = check_single_session_worker
    elif task_name == "joinGroup":
        worker_func = join_group_worker
        args = [config.get("links", [])]
    elif task_name == "seedingGroup":
        worker_func = seeding_group_worker
        args = [config] # Pass whole config
    
    if not worker_func:
        if task_id in TASKS: del TASKS[task_id]
        return jsonify({"error": "Tác vụ không được hỗ trợ"}), 400
    
    # Note the new arguments passed to the thread
    thread = Thread(
        target=run_task_in_thread,
        args=(
            task_id, group_id, group["folder_path"], filenames, core,
            delay_per_session, delay_between_batches, admin_enabled, admin_delay,
            worker_func, *args
        ),
        kwargs={"proxies": proxies_to_use}
    )
    thread.daemon = True
    thread.start()
    return jsonify({"task_id": task_id}), 202


@telegram_bp.route("/api/global-settings", methods=["POST"])
def save_telegram_global_settings():
    """Saves the global Telegram settings (Core, Delays, Admin) to the database."""
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Invalid JSON payload"}), 400

        conn = get_db_connection()
        # Use a targeted UPDATE. We assume the row with id=1 exists.
        # This only touches the columns relevant to the main control bar.
        conn.execute(
            """UPDATE auto_seeding_settings SET
                 core = ?,
                 delay_per_session = ?,
                 delay_between_batches = ?,
                 admin_enabled = ?,
                 admin_delay = ?
               WHERE id = 1;
            """,
            (
                data.get("core", 5),
                data.get("delay_per_session", 10),
                data.get("delay_between_batches", 600),
                data.get("admin_enabled", False),
                data.get("admin_delay", 10)
            )
        )
        conn.commit()
        conn.close()
        return jsonify({"success": True, "message": "Đã lưu cài đặt chung."})

    except sqlite3.Error as e:
        print(f"CRITICAL (SAVE-GLOBAL): Database error: {e}")
        return jsonify({"success": False, "error": f"Database error: {str(e)}"}), 500
    except Exception as e:
        print(f"CRITICAL (SAVE-GLOBAL): General error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@telegram_bp.route("/api/task-status/<task_id>")
def task_status(task_id):
    task = TASKS.get(task_id)
    if not task:
        return jsonify({"status": "not_found"}), 404
    response = task.copy()
    response["results"], response["messages"] = task.get("results", []), task.get(
        "messages", []
    )
    task["results"], task["messages"] = [], []
    return jsonify(response)


@telegram_bp.route("/api/stop-task/<task_id>", methods=["POST"])
def stop_task_route(task_id):
    if task_id in TASKS:
        TASKS[task_id]["status"] = "stopped"
    return jsonify({"message": "Yêu cầu dừng đã được gửi."}), 200


@telegram_bp.route("/api/active-tasks")
def get_active_tasks():
    active_tasks = {
        task_id: {
            "task_name": task_data.get("task_name"),
            "group_id": task_data.get("group_id"),
            "status": task_data.get("status"),
            "total": task_data.get("total"),
            "processed": task_data.get("processed"),
            "success": task_data.get("success"),
            "failed": task_data.get("failed"),
        }
        for task_id, task_data in TASKS.items()
        if task_data.get("status") in ["running", "stopped"]
    }
    return jsonify(active_tasks)


@telegram_bp.route("/api/sessions/delete", methods=["POST"])
def delete_sessions():
    """Delete dead session files from a group's folder and filesystem."""
    import os.path
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "Invalid JSON payload"}), 400
    
    group_id = data.get("group_id")
    filenames = data.get("filenames", [])
    
    # Validate input
    if not group_id:
        return jsonify({"error": "group_id is required"}), 400
    
    if not filenames or not isinstance(filenames, list):
        return jsonify({"error": "filenames must be a non-empty list"}), 400
    
    # Check if any task is currently running (concurrency guard)
    active_tasks = [task for task in TASKS.values() if task.get("status") == "running"]
    if active_tasks:
        return jsonify({"error": "Task is running"}), 409
    
    # Get group info from database
    conn = get_db_connection()
    try:
        group = conn.execute("SELECT folder_path FROM session_groups WHERE id = ?", (group_id,)).fetchone()
        if not group:
            return jsonify({"error": "Group not found"}), 404
        
        group_folder = group["folder_path"]
        if not os.path.exists(group_folder):
            return jsonify({"error": "Group folder not found"}), 404
        
        # Ensure the group folder is within the expected upload folder (security check)
        upload_folder = app.config["UPLOAD_FOLDER"]
        if not group_folder.startswith(upload_folder):
            return jsonify({"error": "Invalid group folder path"}), 400
        
        deleted = []
        missing = []
        failed = []
        
        # Delete each session file
        for filename in filenames:
            # Sanitize filename (prevent path traversal)
            clean_filename = os.path.basename(filename)
            if clean_filename != filename or not clean_filename:
                failed.append(filename)
                continue
            
            file_path = os.path.join(group_folder, clean_filename)
            
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    deleted.append(clean_filename)
                    
                    # Also remove from session_metadata if exists
                    conn.execute("DELETE FROM session_metadata WHERE group_id = ? AND filename = ?", 
                               (group_id, clean_filename))
                else:
                    missing.append(clean_filename)
            except OSError as e:
                failed.append(clean_filename)
                print(f"Failed to delete {file_path}: {e}")
        
        conn.commit()
        
        return jsonify({
            "deleted": deleted,
            "missing": missing, 
            "failed": failed
        })
        
    finally:
        conn.close()


# --- IMAGE EDITOR: BLUEPRINT & LOGIC ---
image_editor_bp = Blueprint("image_editor", __name__, url_prefix="/image-editor")


def resize_crop_image(img, target_w, target_h):
    if target_w <= 0 or target_h <= 0:
        return img
    img_ratio = img.width / img.height
    target_ratio = target_w / target_h
    if img_ratio > target_ratio:
        new_height = target_h
        new_width = int(new_height * img_ratio)
    else:
        new_width = target_w
        new_height = int(new_width / img_ratio)
    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
    left = (new_width - target_w) // 2
    top = (new_height - target_h) // 2
    right = (new_width + target_w) // 2
    bottom = (new_height + target_h) // 2
    return img.crop((left, top, right, bottom))


@image_editor_bp.route("/upload", methods=["POST"])
def upload_images():
    if "files" not in request.files:
        return jsonify({"error": "Không có file nào được gửi"}), 400
    files = request.files.getlist("files")
    if not files or files[0].filename == "":
        return jsonify({"error": "Vui lòng chọn file"}), 400
    session_id = str(uuid.uuid4())
    upload_path = os.path.join(app.config["IMAGE_EDIT_FOLDER"], session_id, "uploads")
    os.makedirs(upload_path, exist_ok=True)
    saved_files = []
    for file in files:
        if file:
            filename = secure_filename(file.filename)
            save_path = os.path.join(upload_path, filename)
            file.save(save_path)
            saved_files.append(f"/image-editor/files/{session_id}/uploads/{filename}")
    return jsonify({"success": True, "files": saved_files, "sessionId": session_id})


@image_editor_bp.route("/create-collage", methods=["POST"])
def create_collage():
    data = request.get_json()
    session_id = data.get("sessionId")
    image_urls = data.get("images")  # URLs tương đối từ client
    layout = data.get("layout")
    spacing = int(data.get("space", 10))
    bg_color = data.get("color", "#1a1d21")
    canvas_w, canvas_h = 1200, 1200

    if not all([session_id, image_urls, layout]):
        return jsonify({"error": "Dữ liệu không hợp lệ"}), 400

    # Định nghĩa các layout
    layouts = {
        # 1 ảnh
        "layout_1_1": [(0, 0, 1, 1)],
        # 2 ảnh
        "layout_2_h": [(0, 0, 0.5, 1), (0.5, 0, 0.5, 1)],
        "layout_2_v": [(0, 0, 1, 0.5), (0, 0.5, 1, 0.5)],
        # 3 ảnh
        "layout_3_top_1": [(0, 0, 1, 0.5), (0, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5)],
        "layout_3_left_1": [(0, 0, 0.5, 1), (0.5, 0, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5)],
        "layout_3_v": [(0, 0, 1 / 3, 1), (1 / 3, 0, 1 / 3, 1), (2 / 3, 0, 1 / 3, 1)],
        # 4 ảnh
        "layout_4_2x2": [
            (0, 0, 0.5, 0.5),
            (0.5, 0, 0.5, 0.5),
            (0, 0.5, 0.5, 0.5),
            (0.5, 0.5, 0.5, 0.5),
        ],
        "layout_4_left_1": [
            (0, 0, 0.5, 1),
            (0.5, 0, 0.5, 1 / 3),
            (0.5, 1 / 3, 0.5, 1 / 3),
            (0.5, 2 / 4, 0.5, 1 / 3),
        ],
        # 5 ảnh
        "layout_5_2_3": [
            (0, 0, 0.5, 0.5), (0.5, 0, 0.5, 0.5),
            (0, 0.5, 1 / 3, 0.5), (1 / 3, 0.5, 1 / 3, 0.5), (2 / 3, 0.5, 1 / 3, 0.5)
        ],
        "layout_5_left_1_4": [
            (0, 0, 2 / 3, 1),
            (2 / 3, 0, 1 / 3, 1 / 4), (2 / 3, 1 / 4, 1 / 3, 1 / 4),
            (2 / 3, 2 / 4, 1 / 3, 1 / 4), (2 / 3, 3 / 4, 1 / 3, 1 / 4)
        ],
    }

    if layout not in layouts:
        return jsonify({"error": f"Layout '{layout}' không tồn tại"}), 400

    layout_coords = layouts.get(layout)
    num_images_needed = len(layout_coords)
    if len(image_urls) < num_images_needed:
        return (
            jsonify(
                {
                    "error": f"Layout này cần {num_images_needed} ảnh, bạn mới cung cấp {len(image_urls)} ảnh."
                }
            ),
            400,
        )

    # Tạo ảnh canvas
    canvas = Image.new("RGB", (canvas_w, canvas_h), bg_color)
    base_path = os.path.join(app.config["IMAGE_EDIT_FOLDER"], session_id, "uploads")

    for i, coords in enumerate(layout_coords):
        if i >= len(image_urls):
            break
        img_url = image_urls[i]
        img_filename = os.path.basename(img_url)
        img_path = os.path.join(base_path, img_filename)

        if not os.path.exists(img_path):
            continue
        img = Image.open(img_path)

        box_w = int(canvas_w * coords[2] - spacing * 1.5)
        box_h = int(canvas_h * coords[3] - spacing * 1.5)
        box_l = int(canvas_w * coords[0]) + spacing
        box_t = int(canvas_h * coords[1]) + spacing
        
        img_resized = resize_crop_image(img, int(box_w), int(box_h))
        canvas.paste(img_resized, (box_l, box_t))

    # Lưu ảnh kết quả
    output_dir = os.path.join(app.config["IMAGE_EDIT_FOLDER"], session_id, "outputs")
    os.makedirs(output_dir, exist_ok=True)
    output_filename = f"collage_{uuid.uuid4().hex[:8]}.jpg"
    output_path = os.path.join(output_dir, output_filename)
    canvas.save(output_path, "JPEG", quality=90)

    result_url = f"/image-editor/files/{session_id}/outputs/{output_filename}"
    return jsonify({"success": True, "result_url": result_url})

# ==> THÊM ROUTE NÀY VÀO <==
@image_editor_bp.route("/files/<path:filepath>")
def serve_image_editor_file(filepath):
    """Gửi các file ảnh (đã tải lên và đã ghép) cho trình duyệt."""
    return send_from_directory(app.config["IMAGE_EDIT_FOLDER"], filepath)


# --- NOTES (REMINDER) MANAGER: BLUEPRINT & LOGIC ---
notes_bp = Blueprint("notes", __name__, url_prefix="/notes")

# Hàng đợi thông báo trong bộ nhớ
NOTIFICATIONS_QUEUE = []

def check_and_queue_reminders():
    """Checks for due reminders from SQLite and queues them for notification."""
    conn = get_db_connection()
    now_utc_iso = datetime.now(timezone.utc).isoformat()
    due_notes = conn.execute(
        "SELECT * FROM notes WHERE status = 'active' AND due_time IS NOT NULL AND due_time <= ?",
        (now_utc_iso,)
    ).fetchall()

    if not due_notes:
        conn.close()
        return

    sound_files = []
    if os.path.exists(SOUNDS_FOLDER):
        sound_files = [f for f in os.listdir(SOUNDS_FOLDER) if f.endswith((".wav", ".mp3", ".ogg"))]
    
    ids_to_update = []
    for note in due_notes:
        note_dict = dict(note)
        title_lower = (note_dict.get("title_html") or "").lower() # Use title_html
        sound_url = "/notes/sounds/notification.wav" # Default
        for sf in sound_files:
            if os.path.splitext(sf)[0].lower() in title_lower:
                sound_url = f"/notes/sounds/{sf}"
                break
        
        notification_payload = {
            "id": note_dict["id"],
            "title": BeautifulSoup(note_dict["title_html"], "html.parser").get_text(),
            "notes": note_dict.get("content_html", ""),
            "sound_url": sound_url
        }
        if not any(q['id'] == note_dict['id'] for q in NOTIFICATIONS_QUEUE):
            NOTIFICATIONS_QUEUE.append(notification_payload)
        
        ids_to_update.append(note_dict["id"])

    if ids_to_update:
        # Update status in bulk
        placeholders = ','.join('?' for _ in ids_to_update)
        conn.execute(f"UPDATE notes SET status = 'notified', due_time = NULL WHERE id IN ({placeholders})", ids_to_update)
        conn.commit()
    
    conn.close()

@notes_bp.route("/api/get")
def api_get_notes():
    check_and_queue_reminders()
    conn = get_db_connection()
    notes_rows = conn.execute("SELECT * FROM notes ORDER BY modified_at DESC").fetchall()
    conn.close()
    return jsonify([dict(row) for row in notes_rows])

@notes_bp.route("/api/add", methods=["POST"])
def api_add_note():
    data = request.json
    title_html, content_html = data.get("title_html", "").strip(), data.get("content_html", "").strip()
    if not title_html and not content_html:
        return jsonify({"error": "Tiêu đề hoặc nội dung không được để trống"}), 400
    
    now = datetime.now(timezone.utc).isoformat()
    new_note = {
        "id": str(uuid.uuid4()), "title_html": title_html, "content_html": content_html,
        "due_time": data.get("due_time") or None, "status": "active" if data.get("due_time") else "none", "modified_at": now
    }

    conn = get_db_connection()
    conn.execute(
        "INSERT INTO notes (id, title_html, content_html, due_time, status, modified_at) VALUES (?, ?, ?, ?, ?, ?)",
        (new_note['id'], new_note['title_html'], new_note['content_html'], new_note['due_time'], new_note['status'], new_note['modified_at'])
    )
    conn.commit()
    conn.close()
    return jsonify(new_note), 201

@notes_bp.route("/api/update/<note_id>", methods=["POST"])
def api_update_note(note_id):
    data = request.json
    title_html, content_html = data.get("title_html", "").strip(), data.get("content_html", "").strip()
    if not title_html and not content_html:
        return jsonify({"error": "Tiêu đề hoặc nội dung không được để trống"}), 400
    
    due_time = data.get("due_time") or None
    status = "active" if due_time else "none" # Simplified status logic, you may need to adjust if "notified" state is handled differently
    modified_at = datetime.now(timezone.utc).isoformat()

    conn = get_db_connection()
    cursor = conn.execute(
        "UPDATE notes SET title_html = ?, content_html = ?, due_time = ?, status = ?, modified_at = ? WHERE id = ?",
        (title_html, content_html, due_time, status, modified_at, note_id)
    )
    conn.commit()
    
    if cursor.rowcount == 0:
        conn.close()
        return jsonify({"error": "Không tìm thấy ghi chú"}), 404
    
    updated_note = conn.execute("SELECT * FROM notes WHERE id = ?", (note_id,)).fetchone()
    conn.close()
    return jsonify(dict(updated_note))

@notes_bp.route("/api/delete/<note_id>", methods=["POST"])
def api_delete_note(note_id):
    conn = get_db_connection()
    cursor = conn.execute("DELETE FROM notes WHERE id = ?", (note_id,))
    conn.commit()
    conn.close()
    return jsonify({"success": True}) if cursor.rowcount > 0 else (jsonify({"error": "Không tìm thấy ghi chú"}), 404)

@notes_bp.route("/api/mark/<note_id>", methods=["POST"])
def api_toggle_mark(note_id):
    conn = get_db_connection()
    note = conn.execute("SELECT is_marked FROM notes WHERE id = ?", (note_id,)).fetchone()
    if not note:
        conn.close()
        return jsonify({"error": "Không tìm thấy ghi chú"}), 404
    
    new_marked_state = not note["is_marked"]
    conn.execute("UPDATE notes SET is_marked = ? WHERE id = ?", (new_marked_state, note_id))
    conn.commit()
    
    updated_note = conn.execute("SELECT * FROM notes WHERE id = ?", (note_id,)).fetchone()
    conn.close()
    return jsonify(dict(updated_note))

@notes_bp.route("/api/acknowledge-notification/<note_id>", methods=["POST"])
def acknowledge_notification(note_id):
    conn = get_db_connection()
    conn.execute("UPDATE notes SET status = 'notified', due_time = NULL WHERE id = ?", (note_id,))
    conn.commit()
    conn.close()
    return jsonify({"success": True})

@notes_bp.route("/api/check-notifications")
def api_check_notifications():
    """Endpoint để frontend kiểm tra xem có thông báo mới không."""
    check_and_queue_reminders()
    if NOTIFICATIONS_QUEUE:
        # Lấy thông báo đầu tiên và xóa khỏi hàng đợi
        notification_to_send = NOTIFICATIONS_QUEUE.pop(0)
        return jsonify(notification_to_send)
    return jsonify(None) # Trả về null nếu không có gì

@notes_bp.route("/sounds/<path:filename>")
def serve_sound(filename):
    return send_from_directory(os.path.join(DATA_DIR, "sounds"), filename)


# --- Inline Editing: Update Telegram Session Info ---
async def update_profile_worker(session_path, field, value, proxy_info):
    """
    Worker to connect to Telegram and update profile (name or username).
    """
    client = None
    result = {"success": False, "error": "Unknown error."}
    try:
        proxy_dict = parse_proxy_string(proxy_info)
        client = TelegramClient(session_path, API_ID, API_HASH, proxy=proxy_dict)
        await client.connect()

        if not await client.is_user_authorized():
            raise Exception("Session is dead or logged out.")

        if field == 'full_name':
            first_name = value.split(' ', 1)[0] if ' ' in value else value
            last_name = value.split(' ', 1)[1] if ' ' in value else ""
            await client(UpdateProfileRequest(first_name=first_name, last_name=last_name))
            result["message"] = "Full name updated successfully."

        elif field == 'username':
            await client(UpdateUsernameRequest(value))
            result["message"] = "Username updated successfully."
        
        me = await client.get_me()
        result.update({
            "success": True,
            "updated_value": me.username if field == 'username' else f"{me.first_name or ''} {me.last_name or ''}".strip(),
            "updated_full_name": f"{me.first_name or ''} {me.last_name or ''}".strip(),
            "updated_username": me.username or "",
            "is_live": True
        })

    except UsernameInvalidError:
        result["error"] = "Invalid username format."
    except UsernameOccupiedError:
        result["error"] = "Username is already taken."
    except FloodWaitError as e:
        result["error"] = f"Flood wait: please wait {e.seconds}s."
    except Exception as e:
        result["error"] = str(e)
    finally:
        if client and client.is_connected():
            await client.disconnect()
    return result

@telegram_bp.route("/api/update-session-info", methods=["POST"])
def update_session_info():
    data = request.get_json()
    group_id, filename, field, value = data.get("group_id"), data.get("filename"), data.get("field"), data.get("value")

    if not all([group_id, filename, field, value]):
        return jsonify({"error": "Missing required data"}), 400
    if field not in ['full_name', 'username']:
        return jsonify({"error": "Invalid field to update"}), 400

    conn = get_db_connection()
    group = conn.execute("SELECT folder_path FROM session_groups WHERE id = ?", (group_id,)).fetchone()
    if not group:
        conn.close()
        return jsonify({"error": "Group not found"}), 404
    
    session_path = os.path.join(group["folder_path"], filename)
    if not os.path.exists(session_path):
        conn.close()
        return jsonify({"error": "Session file not found"}), 404
    
    proxy_config = load_proxies()
    proxy_info = proxy_config['proxies'][0] if proxy_config.get('enabled') and proxy_config.get('proxies') else None

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    update_result = loop.run_until_complete(update_profile_worker(session_path, field, value, proxy_info))
    loop.close()

    if not update_result["success"]:
        conn.close()
        return jsonify({"error": update_result["error"]}), 500

    conn.execute(
        "UPDATE session_metadata SET full_name = ?, username = ?, is_live = ?, status_text = ?, last_checked = CURRENT_TIMESTAMP WHERE group_id = ? AND filename = ?",
        (update_result["updated_full_name"], update_result["updated_username"], True, "Live", group_id, filename)
    )
    conn.commit()
    conn.close()

    return jsonify(update_result)


def manage_startup_shortcut(enable):
    """
    Creates or deletes the application's shortcut in the Windows Startup folder using winshell.
    """
    try:
        # Use winshell to reliably get the path to the user's Startup folder
        startup_folder = winshell.startup()
        
        # Define the path for our shortcut file
        shortcut_path = os.path.join(startup_folder, 'STool Dashboard.lnk')
        
        if enable:
            # --- Create Shortcut Logic ---
            if os.path.exists(shortcut_path):
                print("Startup shortcut already exists.")
                return

            target_script = os.path.abspath(sys.argv[0])
            working_dir = os.path.dirname(target_script)
            
            print(f"Creating startup shortcut for: {target_script}")
            
            with winshell.shortcut(shortcut_path) as shortcut:
                shortcut.path = sys.executable # The Python interpreter
                shortcut.arguments = f'"{target_script}"' # The .pyw script to run
                shortcut.working_directory = working_dir
                shortcut.description = "Start STool Dashboard"
            
            print("Startup shortcut created successfully.")

        else:
            # --- Delete Shortcut Logic ---
            if os.path.exists(shortcut_path):
                print("Deleting startup shortcut.")
                os.remove(shortcut_path)
                print("Startup shortcut deleted.")
            else:
                print("Startup shortcut does not exist, nothing to delete.")

    except Exception as e:
        # Log any errors that occur during shortcut management
        print(f"ERROR managing startup shortcut: {e}")


# --- DASHBOARD SETTINGS MANAGER: BLUEPRINT & LOGIC ---
dashboard_bp = Blueprint("dashboard", __name__, url_prefix="/dashboard")

def load_dashboard_settings():
    if not os.path.exists(DASHBOARD_SETTINGS_FILE):
        with open(DASHBOARD_SETTINGS_FILE, "w", encoding="utf-8") as f:
            json.dump({"auto_start": False}, f)
        return {"auto_start": False}
    with open(DASHBOARD_SETTINGS_FILE, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except json.JSONDecodeError:
            return {"auto_start": False}

def save_dashboard_settings(settings):
    with open(DASHBOARD_SETTINGS_FILE, "w", encoding="utf-8") as f:
        json.dump(settings, f, indent=4, ensure_ascii=False)

@dashboard_bp.route("/api/settings", methods=["GET"])
def get_dashboard_settings():
    settings = load_dashboard_settings()
    return jsonify(settings)

@dashboard_bp.route("/api/settings", methods=["POST"])
def set_dashboard_settings():
    data = request.json
    settings = load_dashboard_settings()
    
    # Get the new auto_start value from the request
    new_auto_start_state = data.get("auto_start", False)
    settings["auto_start"] = new_auto_start_state
    
    # Save the setting to the JSON file first
    save_dashboard_settings(settings)
    
    # THEN, call the function to create or delete the shortcut based on the new state
    manage_startup_shortcut(new_auto_start_state)
    
    return jsonify({"success": True})


# --- UTILITIES MANAGER: BLUEPRINT & LOGIC ---
utilities_bp = Blueprint("utilities", __name__, url_prefix="/utilities")

@utilities_bp.route("/shutdown", methods=["POST"])
def schedule_shutdown():
    """Schedules a system shutdown on Windows."""
    data = request.json
    try:
        seconds = int(data.get("seconds", 0))
        if seconds <= 0:
            return jsonify({"error": "Số giây phải lớn hơn 0"}), 400

        # Command to cancel any existing shutdown timer first, then set a new one.
        os.system("shutdown /a") 
        command = f"shutdown /s /t {seconds}"
        os.system(command)
        
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        message = f"Đã hẹn giờ tắt máy sau {hours} giờ {minutes} phút."
        return jsonify({"success": True, "message": message})
    except (ValueError, TypeError):
        return jsonify({"error": "Dữ liệu không hợp lệ"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@utilities_bp.route("/cancel-shutdown", methods=["POST"])
def cancel_shutdown():
    """Cancels any pending system shutdown on Windows."""
    try:
        # The /a switch aborts a system shutdown.
        os.system("shutdown /a")
        return jsonify({"success": True, "message": "Đã hủy lệnh hẹn giờ tắt máy."})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# --- AUTOMATIC SEEDING MANAGER: BLUEPRINT & LOGIC (SIMPLIFIED) ---

automatic_bp = Blueprint("automatic", __name__, url_prefix="/automatic")

# This is the background thread that checks the schedule.
def scheduler_thread_worker():
    """Background worker that checks if the current time is within the scheduled window and triggers the task once per day."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    while True:
        try:
            conn = get_db_connection()
            settings = conn.execute("SELECT * FROM auto_seeding_settings WHERE id = 1").fetchone()
            
            if not settings or not settings['is_enabled'] or not settings['run_time'] or not settings['end_run_time']:
                conn.close()
                time.sleep(30)
                continue

            now = datetime.now()
            now_time = now.time()
            
            start_time = datetime.strptime(settings['run_time'], "%H:%M").time()
            end_time = datetime.strptime(settings['end_run_time'], "%H:%M").time()

            is_in_window = False
            if start_time <= end_time: # Normal case (e.g., 09:00 to 17:00)
                if start_time <= now_time < end_time:
                    is_in_window = True
            else: # Overnight case (e.g., 22:00 to 02:00)
                if now_time >= start_time or now_time < end_time:
                    is_in_window = True

            if is_in_window:
                last_run_str = settings['last_run_timestamp']
                should_run = False
                if not last_run_str:
                    should_run = True # Never run before
                else:
                    last_run_dt = datetime.fromisoformat(last_run_str)
                    # ROBUST FIX: Check if the last run was more than 22 hours ago.
                    # This reliably prevents re-triggering within the same 24-hour cycle for any schedule type.
                    if now - last_run_dt > timedelta(hours=22):
                        should_run = True
                
                if should_run:
                    print(f"[{datetime.now()}] Auto Seeding: Conditions met. Triggering task.")
                    loop.run_until_complete(trigger_auto_seeding_task())
                    
                    current_run_timestamp = now.isoformat()
                    conn.execute("UPDATE auto_seeding_settings SET last_run_timestamp = ? WHERE id = 1", (current_run_timestamp,))
                    conn.commit()

                    if not settings['run_daily']:
                        conn.execute("UPDATE auto_seeding_settings SET is_enabled = 0 WHERE id = 1")
                        conn.commit()
                        print("Auto Seeding: Run once completed, is_enabled set to false.")
            
            conn.close()
        except Exception as e:
            print(f"Error in Auto Seeding scheduler thread: {e}")
        
        time.sleep(30)

async def trigger_auto_seeding_task():
    """Fetches ALL configs from DB, prepares data, and starts the seeding task with the correct parameters."""
    print(f"[{datetime.now()}] Triggering Auto Seeding task...")
    
    conn = get_db_connection()
    # Fetch ALL settings for auto-seeding
    settings = conn.execute("SELECT * FROM auto_seeding_settings WHERE id = 1").fetchone()
    if not settings or not settings['target_session_group_id']:
        print("Auto Seeding Error: Target session group is not configured.")
        conn.close()
        return

    target_group_id = settings['target_session_group_id']
    group_info = conn.execute("SELECT folder_path FROM session_groups WHERE id = ?", (target_group_id,)).fetchone()
    if not group_info:
        print(f"Auto Seeding Error: Session group with ID {target_group_id} not found.")
        conn.close()
        return
    group_folder_path = group_info['folder_path']
    
    # Get the seedingGroup task config (messages, links, etc.)
    seeding_config_row = conn.execute("SELECT config_json FROM task_configs WHERE task_name = 'seedingGroup'").fetchone()
    if not seeding_config_row:
        print("Auto Seeding Error: 'seedingGroup' configuration card not found.")
        conn.close()
        return

    config = json.loads(seeding_config_row['config_json'])
    messages = config.get("messages", [])
    if not messages:
        print("Auto Seeding Error: No messages found in seeding config.")
        conn.close()
        return
        
    random.shuffle(messages)
    config["messages"] = messages
    
    session_files = [f for f in os.listdir(group_folder_path) if f.endswith(".session")]
    if not session_files:
        print(f"Auto Seeding Error: No session files found in the target group folder.")
        conn.close()
        return
    random.shuffle(session_files)
    conn.close()

    task_id = str(uuid.uuid4())
    TASKS[task_id] = {
        "task_name": "seedingGroup", "group_id": target_group_id, "status": "running",
        "total": len(session_files), "processed": 0, "success": 0, "failed": 0,
        "results": [], "messages": []
    }
    
    # Use the parameters fetched from the settings table instead of hardcoded values
    thread = Thread(
        target=run_task_in_thread,
        args=(
            task_id, target_group_id, group_folder_path, session_files,
            settings['core'], settings['delay_per_session'], settings['delay_between_batches'],
            settings['admin_enabled'], settings['admin_delay'],
            seeding_group_worker, config
        ),
        kwargs={"proxies": []} # Auto seeding will not use proxy for now
    )
    thread.daemon = True
    thread.start()
    print(f"Auto Seeding task {task_id} started for group {target_group_id} with {len(session_files)} sessions.")

@automatic_bp.route("/api/seeding/settings", methods=["GET"])
def get_auto_seeding_settings():
    conn = get_db_connection()
    settings = conn.execute("SELECT * FROM auto_seeding_settings WHERE id = 1").fetchone()
    conn.close()
    
    # --- START: NEW DEBUG LOGIC ---
    settings_dict = dict(settings) if settings else {}
    print(f"DEBUG: Data fetched by GET request (on tab load): {settings_dict}")
    # --- END: NEW DEBUG LOGIC ---
    
    return jsonify(settings_dict)

@automatic_bp.route("/api/seeding/settings", methods=["POST"])
def save_auto_seeding_settings():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Invalid JSON payload"}), 400

        conn = get_db_connection()
        conn.execute(
            """INSERT INTO auto_seeding_settings (id, is_enabled, run_time, end_run_time, run_daily, target_session_group_id, task_name, core, delay_per_session, delay_between_batches, admin_enabled, admin_delay)
               VALUES (1, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
               ON CONFLICT(id) DO UPDATE SET
                 is_enabled = excluded.is_enabled,
                 run_time = excluded.run_time,
                 end_run_time = excluded.end_run_time,
                 run_daily = excluded.run_daily,
                 target_session_group_id = excluded.target_session_group_id,
                 task_name = excluded.task_name,
                 core = excluded.core,
                 delay_per_session = excluded.delay_per_session,
                 delay_between_batches = excluded.delay_between_batches,
                 admin_enabled = excluded.admin_enabled,
                 admin_delay = excluded.admin_delay;
            """,
            (
                data.get("is_enabled", False),
                data.get("run_time"),
                data.get("end_run_time"),
                data.get("run_daily", False),
                data.get("target_session_group_id"),
                data.get("task_name", "seedingGroup"),
                data.get("core", 5),
                data.get("delay_per_session", 10),
                data.get("delay_between_batches", 600),
                data.get("admin_enabled", False),
                data.get("admin_delay", 10)
            )
        )
        conn.commit()
        conn.close()
        return jsonify(data)

    except sqlite3.Error as e:
        print(f"CRITICAL (SAVE): Database error in save_auto_seeding_settings: {e}")
        return jsonify({"success": False, "error": f"Database error: {str(e)}"}), 500
    except Exception as e:
        print(f"CRITICAL (SAVE): General error in save_auto_seeding_settings: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


# --- MAIN APP ROUTES & SETUP ---
app.register_blueprint(password_bp)
app.register_blueprint(facebook_bp)
app.register_blueprint(telegram_bp)
app.register_blueprint(image_editor_bp)
app.register_blueprint(notes_bp)
app.register_blueprint(dashboard_bp)
app.register_blueprint(utilities_bp)
app.register_blueprint(automatic_bp)

# Global scheduler thread variable
scheduler_thread = None

# Start scheduler thread for automatic seeding
def start_scheduler_thread():
    """Start the scheduler thread for automatic seeding."""
    global scheduler_thread
    if scheduler_thread is None or not scheduler_thread.is_alive():
        scheduler_thread = Thread(target=scheduler_thread_worker, daemon=True)
        scheduler_thread.start()
        print("Auto seeding scheduler thread started")

# Disable caching để luôn reload static files
@app.after_request
def after_request(response):
    response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
    response.headers["Pragma"] = "no-cache"
    response.headers["Expires"] = "0"
    return response


@app.route("/")
def main_dashboard():
    # Fetch password accounts from SQLite
    conn = get_db_connection()
    password_accounts_rows = conn.execute("SELECT * FROM passwords ORDER BY type, username").fetchall()
    password_accounts = [dict(row) for row in password_accounts_rows]
    
    # Fetch password types from SQLite
    password_types_rows = conn.execute("SELECT * FROM password_types ORDER BY name").fetchall()
    password_types = [dict(row) for row in password_types_rows]
    conn.close()
    
    facebook_accounts = load_fb_accounts()
    filter_type = request.args.get("type", "")
    if filter_type:
        password_accounts = [
            acc for acc in password_accounts if acc.get("type") == filter_type
        ]

    return render_template(
        "index.html",
        password_accounts=sorted(
            password_accounts,
            key=lambda x: (x.get("type", "").lower(), x.get("username", "").lower()),
        ),
        password_types=password_types,
        facebook_accounts=sorted(
            facebook_accounts, key=lambda x: x.get("nickname", "").lower()
        ),
        request=request,
    )

@app.route('/static/<path:filename>')
def static_files(filename):
    """Serve static files với no-cache headers"""
    response = send_from_directory('static', filename)
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    return response


def open_browser():
    """Mở trang dashboard trong trình duyệt web mặc định."""
    try:
        webbrowser.open_new("http://127.0.0.1:5000/")
    except Exception as e:
        print(f"Could not open browser: {e}")


def run_flask_app():
    """Hàm chạy Flask app trong một luồng riêng."""
    # Không mở trình duyệt tự động khi ở chế độ debug
    if not app.debug:
        Thread(target=lambda: (time.sleep(2), open_browser())).start()
    port = int(os.environ.get("PORT", 5000))
    print(f"STool Dashboard is running on http://127.0.0.1:{port}")
    print("Debug mode: ON - Tự động reload khi thay đổi code")
    # Bật debug nhưng tắt reloader vì chạy trong thread
    app.run(debug=True, host="0.0.0.0", port=port, use_reloader=False)


def exit_action(icon, item):
    """Hàm xử lý khi nhấn nút thoát trên tray icon."""
    print("Exiting application...")
    icon.stop()
    # os._exit(0) là một cách để buộc thoát toàn bộ chương trình,
    # hữu ích khi các luồng con (như Flask) không tự dừng lại.
    os._exit(0)


def setup_and_run_tray():
    """Thiết lập và chạy icon trên khay hệ thống."""
    # Chạy Flask trong một luồng nền, daemon=True để nó tự tắt khi chương trình chính thoát
    flask_thread = Thread(target=run_flask_app, daemon=True)
    flask_thread.start()
    
    # Tải ảnh icon từ file bạn đã cung cấp
    try:
        image = Image.open(os.path.join(app.root_path, "static", "icontray.png"))
    except FileNotFoundError:
        print("Không tìm thấy file 'image_fe55d8.png'. Tạo icon mặc định.")
        width, height = 64, 64
        color1, color2 = (13, 110, 253), (255, 255, 255)
        image = Image.new('RGB', (width, height), color1)
        dc = ImageDraw.Draw(image)
        dc.rectangle((width // 2, 0, width, height // 2), fill=color2)
        dc.rectangle((0, height // 2, width // 2, height), fill=color2)

    # Tạo menu cho icon (default=True nghĩa là nháy đúp sẽ thực hiện hành động này)
    menu = (pystray.MenuItem('Mở Dashboard', open_browser, default=True), pystray.MenuItem('Thoát', exit_action))
    
    # Tạo đối tượng icon
    icon = pystray.Icon("stool_dashboard", image, "STool Dashboard", menu)
    
    # Chạy icon trên luồng chính (lệnh này sẽ blocking, giữ chương trình sống)
    icon.run()


def run_diagnostic_test():
    """
    TEMPORARY DIAGNOSTIC FUNCTION.
    Deletes any previous test record and inserts a new one to verify DB connection.
    """
    print("--- RUNNING DATABASE DIAGNOSTIC TEST ---")
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Define test data
        test_type = 'DIAGNOSTIC'
        test_username = 'TEST_USER_001'
        test_password = 'test_password'
        test_notes = 'If you can see this record in the UI, the database connection is working correctly.'

        # Ensure the 'DIAGNOSTIC' type exists to avoid foreign key errors
        cursor.execute("INSERT OR IGNORE INTO password_types (name, color) VALUES (?, ?)", (test_type, '#ffc107'))

        # Clean up any previous test record to ensure a clean test
        cursor.execute("DELETE FROM passwords WHERE username = ?", (test_username,))
        
        # Insert the new test record
        cursor.execute(
            "INSERT INTO passwords (type, username, password, notes) VALUES (?, ?, ?, ?)",
            (test_type, test_username, test_password, test_notes)
        )
        
        conn.commit()
        
        # Verify insertion
        inserted_row = cursor.execute("SELECT * FROM passwords WHERE username = ?", (test_username,)).fetchone()
        conn.close()

        if inserted_row:
            print(f"--- DIAGNOSTIC: Successfully inserted test user '{test_username}' into Data.db. ---")
        else:
            print("--- DIAGNOSTIC: FAILED to insert or verify test user. ---")

    except Exception as e:
        print(f"--- DIAGNOSTIC: An error occurred during the test: {e} ---")


def migrate_legacy_telegram_db():
    """
    Performs a one-time migration of data from the old telegram_dashboard.db
    to the new consolidated Data.db.
    """
    legacy_db_path = os.path.join(DATA_DIR, "telegram_dashboard.db")
    main_db_path = DATABASE_PATH

    # If the old database doesn't exist, do nothing.
    if not os.path.exists(legacy_db_path):
        return

    print("--- Found legacy telegram_dashboard.db. Starting migration... ---")

    try:
        legacy_conn = sqlite3.connect(legacy_db_path)
        legacy_conn.row_factory = sqlite3.Row
        legacy_cursor = legacy_conn.cursor()

        main_conn = get_db_connection() # Uses the existing function to connect to Data.db
        main_cursor = main_conn.cursor()

        # --- Migrate Groups (Table: session_groups) ---
        legacy_groups = legacy_cursor.execute("SELECT id, name, folder_path FROM groups").fetchall()
        if legacy_groups:
            main_cursor.executemany(
                "INSERT OR IGNORE INTO session_groups (id, name, folder_path) VALUES (?, ?, ?)",
                [(g['id'], g['name'], g['folder_path']) for g in legacy_groups]
            )
            print(f"Migrated {len(legacy_groups)} groups.")

        # --- Migrate Sessions (Table: session_metadata) ---
        legacy_sessions = legacy_cursor.execute("SELECT group_id, filename, phone, full_name, username, is_live, status_text FROM sessions").fetchall()
        if legacy_sessions:
            # Prepare data for insertion into the new table structure
            sessions_to_insert = [
                (
                    s['group_id'], s['filename'], s.get('full_name'), s.get('username'),
                    s.get('is_live'), s.get('status_text'), datetime.now(timezone.utc).isoformat()
                ) for s in legacy_sessions
            ]
            main_cursor.executemany(
                "INSERT OR IGNORE INTO session_metadata (group_id, filename, full_name, username, is_live, status_text, last_checked) VALUES (?, ?, ?, ?, ?, ?, ?)",
                sessions_to_insert
            )
            print(f"Migrated {len(legacy_sessions)} session metadata records.")

        main_conn.commit()
        print("--- Migration successful. ---")

    except sqlite3.Error as e:
        # It's possible the old DB has a different schema. We'll catch the error.
        # This can happen if table 'groups' or 'sessions' does not exist.
        print(f"!!! MIGRATION SKIPPED: An error occurred while accessing telegram_dashboard.db: {e}")
        print("!!! This usually means the file is from an incompatible version or is empty/corrupt.")

    finally:
        if 'legacy_conn' in locals():
            legacy_conn.close()
        if 'main_conn' in locals():
            main_conn.close()

    # Rename the old file to prevent running this again. This is safer than deleting.
    try:
        os.rename(legacy_db_path, legacy_db_path + ".migrated")
        print(f"--- Renamed old database to {os.path.basename(legacy_db_path)}.migrated ---")
    except OSError as e:
        print(f"Could not rename legacy database file: {e}")


def migrate_auto_seeding_schema():
    """
    Performs a one-time, robust migration of the auto_seeding_settings table
    to clean up old/deprecated columns and ensure schema consistency.
    """
    migration_flag = os.path.join(DATA_DIR, 'auto_seeding_schema_v2.flag')
    if os.path.exists(migration_flag):
        return # Migration has already been performed.

    print("--- Checking for and running auto_seeding_settings schema migration... ---")
    conn = get_db_connection()
    try:
        cursor = conn.cursor()
        cursor.execute("PRAGMA table_info(auto_seeding_settings)")
        existing_columns = [row['name'] for row in cursor.fetchall()]

        # Define the clean, correct set of columns we want to keep.
        final_columns = ['id', 'is_enabled', 'run_time', 'end_run_time', 'run_daily', 'target_session_group_id', 'last_run_timestamp', 'task_name', 'core', 'delay_per_session', 'delay_between_batches', 'admin_enabled', 'admin_delay']
        
        # Determine which of the final columns actually exist in the old table to be copied.
        columns_to_copy = [col for col in final_columns if col in existing_columns]
        columns_to_copy_str = ", ".join(columns_to_copy)

        print(f"Schema Migration: The following columns will be preserved: {columns_to_copy_str}")

        # 1. Create a new table with the perfect, clean schema.
        cursor.execute("""
            CREATE TABLE auto_seeding_settings_new (
                id INTEGER PRIMARY KEY,
                is_enabled BOOLEAN NOT NULL DEFAULT 0,
                run_time TEXT,
                end_run_time TEXT,
                run_daily BOOLEAN NOT NULL DEFAULT 0,
                target_session_group_id INTEGER,
                last_run_timestamp TEXT,
                task_name TEXT NOT NULL DEFAULT 'seedingGroup',
                core INTEGER NOT NULL DEFAULT 5,
                delay_per_session INTEGER NOT NULL DEFAULT 10,
                delay_between_batches INTEGER NOT NULL DEFAULT 600,
                admin_enabled BOOLEAN NOT NULL DEFAULT 0,
                admin_delay INTEGER NOT NULL DEFAULT 10
            )
        """)

        # 2. Copy data from the old table to the new one, only selecting the valid columns that exist.
        if columns_to_copy:
            cursor.execute(f"""
                INSERT INTO auto_seeding_settings_new ({columns_to_copy_str})
                SELECT {columns_to_copy_str} FROM auto_seeding_settings
            """)
            print("Schema Migration: Data copied to new clean table.")

        # 3. Drop the old messy table.
        cursor.execute("DROP TABLE auto_seeding_settings")
        print("Schema Migration: Old table dropped.")

        # 4. Rename the new table to the original name.
        cursor.execute("ALTER TABLE auto_seeding_settings_new RENAME TO auto_seeding_settings")
        print("Schema Migration: New table renamed.")
        
        conn.commit()
        
        # 5. Create the flag file to prevent this migration from running again.
        with open(migration_flag, 'w') as f:
            f.write(datetime.now().isoformat())
        print("--- Schema migration for auto_seeding_settings complete. ---")

    except sqlite3.Error as e:
        print(f"!!! MIGRATION SKIPPED for auto_seeding_settings due to an error: {e}")
        # Rollback any partial changes if an error occurs.
        conn.rollback()
    finally:
        conn.close()


if __name__ == "__main__":
    init_database()
    migrate_auto_seeding_schema() # <-- ADD THIS LINE
    migrate_legacy_telegram_db() # <-- ADD THIS LINE
    run_diagnostic_test() # <-- ADD THIS LINE
    migrate_json_to_sqlite()  # Run migration once
    start_scheduler_thread()  # Start automatic seeding scheduler
    setup_and_run_tray()